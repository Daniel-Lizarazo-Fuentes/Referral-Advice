{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sklearn\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "k_folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 897 entries, 0 to 1545\n",
      "Data columns (total 37 columns):\n",
      " #   Column                            Non-Null Count  Dtype  \n",
      "---  ------                            --------------  -----  \n",
      " 0   Treatment                         897 non-null    int64  \n",
      " 1   Fever                             879 non-null    float64\n",
      " 2   Duration_of_pain                  877 non-null    float64\n",
      " 3   Sick_leave                        897 non-null    int64  \n",
      " 4   Earlier_hospitalization           897 non-null    int64  \n",
      " 5   Workoverload                      255 non-null    float64\n",
      " 6   Familiy_history                   897 non-null    int64  \n",
      " 7   Depression                        897 non-null    int64  \n",
      " 8   Extremely_nervous                 862 non-null    float64\n",
      " 9   Stress                            897 non-null    int64  \n",
      " 10  Relationship_with_colleagues      581 non-null    float64\n",
      " 11  Irrational_thoughts_risk_lasting  857 non-null    float64\n",
      " 12  Irrational_thoughts_work          785 non-null    float64\n",
      " 13  Coping_strategy                   851 non-null    float64\n",
      " 14  Kinesiophobia_physical_exercise   866 non-null    float64\n",
      " 15  Kinesiophobia_pain_stop           855 non-null    float64\n",
      " 16  Age                               897 non-null    object \n",
      " 17  Uses_analgesics                   897 non-null    int64  \n",
      " 18  Uses_corticosteroids              888 non-null    float64\n",
      " 19  Serious_disease                   892 non-null    float64\n",
      " 20  Neurogenic_signals                897 non-null    int64  \n",
      " 21  Continuous_pain                   897 non-null    int64  \n",
      " 22  Decreased_mobility                897 non-null    int64  \n",
      " 23  Nocturnal_pain                    897 non-null    int64  \n",
      " 24  Weightloss_per_year               874 non-null    float64\n",
      " 25  Loss_muscle_strength              859 non-null    float64\n",
      " 26  Trauma                            456 non-null    float64\n",
      " 27  Failure_symptoms                  897 non-null    int64  \n",
      " 28  Incoordination                    778 non-null    float64\n",
      " 29  neck_pain_intensity               897 non-null    int64  \n",
      " 30  low_back_pain_intensity           897 non-null    int64  \n",
      " 31  arm_left_pain_intensity           897 non-null    int64  \n",
      " 32  arm_right_pain_intensity          897 non-null    int64  \n",
      " 33  leg_left_pain_intensity           897 non-null    int64  \n",
      " 34  leg_right_pain_intensity          897 non-null    int64  \n",
      " 35  working_ability                   303 non-null    float64\n",
      " 36  Paidwork                          897 non-null    int64  \n",
      "dtypes: float64(17), int64(19), object(1)\n",
      "memory usage: 266.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "xls = pd.ExcelFile('Dataset - LBP RA.xlsx')\n",
    "dataframe = pd.read_excel(xls, 'Training Dataset')\n",
    "#dataframe = dataframe[(dataframe[\"Treatment\"] == 1) | (dataframe[\"Treatment\"] == 5)]\n",
    "dataframe = dataframe[(dataframe[\"Treatment\"] != 5)]\n",
    "dataframe_original = dataframe.copy(True)\n",
    "print(dataframe.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\"Treatment\", \"Weightloss_per_year\"]\n",
    "\n",
    "boolean_columns = [\n",
    "    \"Fever\",\n",
    "    \"Sick_leave\",\n",
    "    \"Earlier_hospitalization\",\n",
    "    \"Workoverload\",\n",
    "    \"Familiy_history\",\n",
    "    \"Depression\",\n",
    "    \"Stress\",\n",
    "    \"Uses_analgesics\",\n",
    "    \"Uses_corticosteroids\",\n",
    "    \"Serious_disease\",\n",
    "    \"Neurogenic_signals\",\n",
    "    \"Continuous_pain\",\n",
    "    \"Nocturnal_pain\",\n",
    "    \"Loss_muscle_strength\",\n",
    "    \"Trauma\",\n",
    "    \"Failure_symptoms\",\n",
    "    \"Incoordination\",\n",
    "    \"Paidwork\",\n",
    "]\n",
    "\n",
    "ordinal_columns = [\n",
    "    \"Duration_of_pain\",\n",
    "    \"Extremely_nervous\",\n",
    "    \"Relationship_with_colleagues\",\n",
    "    \"Irrational_thoughts_risk_lasting\",\n",
    "    \"Irrational_thoughts_work\",\n",
    "    \"Coping_strategy\",\n",
    "    \"Kinesiophobia_physical_exercise\",\n",
    "    \"Kinesiophobia_pain_stop\",\n",
    "    \"Age\",\n",
    "    \"neck_pain_intensity\",\n",
    "    \"low_back_pain_intensity\",\n",
    "    \"arm_left_pain_intensity\",\n",
    "    \"arm_right_pain_intensity\",\n",
    "    \"leg_left_pain_intensity\",\n",
    "    \"leg_right_pain_intensity\",\n",
    "    \"working_ability\",\n",
    "]\n",
    "\n",
    "value_columns = [\"Decreased_mobility\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping integer colum\n",
    "dataframe[value_columns] = dataframe[value_columns].astype(\"Int64\")\n",
    "\n",
    "# Mapping categories and boolean columns\n",
    "dataframe[categorical_columns] = dataframe[categorical_columns].astype(\"category\")\n",
    "dataframe[boolean_columns] = dataframe[boolean_columns].astype(\"boolean\")\n",
    "\n",
    "# Mapping ordinal columns \n",
    "age_mapping = {\n",
    "    \"0-19\": 0,\n",
    "    \"20-29\": 1,\n",
    "    \"30-39\": 2,\n",
    "    \"40-49\": 3,\n",
    "    \"50-59\": 4,\n",
    "    \"60-69\": 5,\n",
    "    \"70-79\": 6,\n",
    "    \">=80\": 7,\n",
    "}\n",
    "\n",
    "dataframe[\"Age\"] = dataframe[\"Age\"].replace(age_mapping)\n",
    "\n",
    "for column in ordinal_columns:\n",
    "    dataframe[[column]] = dataframe[[column]].astype(\"Int64\")\n",
    "    dataframe[column].fillna(-1, inplace=True)\n",
    "    dataframe[column] = pd.Categorical(dataframe[column], categories=sorted(dataframe[column].unique()), ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ada Boosted Decision Tree: Accuracy: 0.5299 (+/- 0.3515)\n",
      "Random Forest: Accuracy: 0.6425 (+/- 0.3703)\n",
      "Gradient Boosting: Accuracy: 0.6135 (+/- 0.3811)\n",
      "KNN: Accuracy: 0.6823 (+/- 0.0361)\n",
      "Logistic Regression: Accuracy: 0.6135 (+/- 0.3886)\n",
      "Neural Network: Accuracy: 0.5799 (+/- 0.1978)\n",
      "SVM: Accuracy: 0.7347 (+/- 0.0046)\n",
      "Average Accuracy Across All Models: 0.6280\n",
      "Ensemble Model (Voting): Accuracy: 0.6647 (+/- 0.2593)\n"
     ]
    }
   ],
   "source": [
    "missing_percentages = dataframe_original.isnull().mean()\n",
    "columns_to_remove = missing_percentages[missing_percentages > 0.7].index.tolist()\n",
    "dataframe = dataframe.drop(columns=columns_to_remove)\n",
    "\n",
    "categorical_columns = [\n",
    "    col for col in categorical_columns if col not in columns_to_remove\n",
    "]\n",
    "ordinal_columns = [col for col in ordinal_columns if col not in columns_to_remove]\n",
    "boolean_columns = [col for col in boolean_columns if col not in columns_to_remove]\n",
    "value_columns = [col for col in value_columns if col not in columns_to_remove]\n",
    "\n",
    "X = dataframe.drop(columns=[\"Treatment\"])\n",
    "imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "y = dataframe[\"Treatment\"]\n",
    "\n",
    "minority_data = dataframe[(dataframe[\"Treatment\"] != 1) & (dataframe[\"Treatment\"] != 5)]\n",
    "minority_data = pd.concat([minority_data] * 3)\n",
    "minority_X = minority_data.drop(columns=[\"Treatment\"])\n",
    "minority_y = minority_data[\"Treatment\"]\n",
    "train_X = imputer.fit_transform(pd.concat([minority_X, X], axis=0))\n",
    "train_y = pd.concat([minority_y, y], axis=0)\n",
    "\n",
    "param_grid_adaboost = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"learning_rate\": [0.1, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "param_grid_random_forest = {\n",
    "    \"estimator__n_estimators\": [50, 100, 200],\n",
    "    \"estimator__max_depth\": [None, 10, 20]\n",
    "}\n",
    "\n",
    "param_grid_gradient_boosting = {\n",
    "    \"estimator__max_iter\": [50, 100, 200],\n",
    "    \"estimator__learning_rate\": [0.01, 0.1, 0.5]\n",
    "}\n",
    "\n",
    "param_grid_knn = {\n",
    "    \"n_neighbors\": [3, 5, 10]\n",
    "}\n",
    "\n",
    "param_grid_logistic_regression = {\n",
    "    \"C\": [0.1, 1.0, 10.0],\n",
    "    \"penalty\": [\"l1\", \"l2\"]\n",
    "}\n",
    "\n",
    "param_grid_neural_network = {\n",
    "    \"hidden_layer_sizes\": [(100,), (50, 50), (50, 100, 50)],\n",
    "    \"alpha\": [0.0001, 0.001, 0.01]\n",
    "}\n",
    "\n",
    "param_grid_svm = {\n",
    "    \"C\": [0.1, 1.0, 10.0],\n",
    "    \"kernel\": [\"linear\", \"rbf\", \"poly\"]\n",
    "}\n",
    "\n",
    "# Perform grid searchfor each model\n",
    "grid_search_adaboost = GridSearchCV(\n",
    "    AdaBoostClassifier(DecisionTreeClassifier()),\n",
    "    param_grid_adaboost,\n",
    "    cv=k_folds,\n",
    "    scoring=\"accuracy\"\n",
    ")\n",
    "grid_search_adaboost.fit(train_X, train_y)\n",
    "\n",
    "grid_search_random_forest = GridSearchCV(\n",
    "    OneVsRestClassifier(RandomForestClassifier()),\n",
    "    param_grid_random_forest,\n",
    "    cv=k_folds,\n",
    "    scoring=\"accuracy\"\n",
    ")\n",
    "grid_search_random_forest.fit(train_X, train_y)\n",
    "\n",
    "grid_search_gradient_boosting = GridSearchCV(\n",
    "    OneVsRestClassifier(HistGradientBoostingClassifier()),\n",
    "    param_grid_gradient_boosting,\n",
    "    cv=k_folds,\n",
    "    scoring=\"accuracy\"\n",
    ")\n",
    "grid_search_gradient_boosting.fit(train_X, train_y)\n",
    "\n",
    "grid_search_knn = GridSearchCV(\n",
    "    KNeighborsClassifier(),\n",
    "    param_grid_knn,\n",
    "    cv=k_folds,\n",
    "    scoring=\"accuracy\"\n",
    ")\n",
    "grid_search_knn.fit(train_X, train_y)\n",
    "\n",
    "grid_search_logistic_regression = GridSearchCV(\n",
    "    LogisticRegression(max_iter=4000, solver=\"saga\"),\n",
    "    param_grid_logistic_regression,\n",
    "    cv=k_folds,\n",
    "    scoring=\"accuracy\"\n",
    ")\n",
    "grid_search_logistic_regression.fit(train_X, train_y)\n",
    "\n",
    "grid_search_neural_network = GridSearchCV(\n",
    "    MLPClassifier(max_iter=4000),\n",
    "    param_grid_neural_network,\n",
    "    cv=k_folds,\n",
    "    scoring=\"accuracy\"\n",
    ")\n",
    "grid_search_neural_network.fit(train_X, train_y)\n",
    "\n",
    "grid_search_svm = GridSearchCV(\n",
    "    SVC(),\n",
    "    param_grid_svm,\n",
    "    cv=k_folds,\n",
    "    scoring=\"accuracy\"\n",
    ")\n",
    "grid_search_svm.fit(train_X, train_y)\n",
    "\n",
    "models = [\n",
    "    (\"Ada Boosted Decision Tree\", grid_search_adaboost.best_estimator_),\n",
    "    (\"Random Forest\", grid_search_random_forest.best_estimator_),\n",
    "    (\"Gradient Boosting\", grid_search_gradient_boosting.best_estimator_),\n",
    "    (\"KNN\", grid_search_knn.best_estimator_),\n",
    "    (\"Logistic Regression\", grid_search_logistic_regression.best_estimator_),\n",
    "    (\"Neural Network\", grid_search_neural_network.best_estimator_),\n",
    "    (\"SVM\", grid_search_svm.best_estimator_)\n",
    "]\n",
    "\n",
    "model_scores = {}\n",
    "for name, model in models:\n",
    "    scores = cross_val_score(model, X_imputed, y, cv=k_folds, scoring=\"accuracy\")\n",
    "    model_scores[name] = scores\n",
    "    print(f\"{name}: Accuracy: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})\")\n",
    "\n",
    "average_accuracy = np.mean([scores.mean() for scores in model_scores.values()])\n",
    "print(f\"Average Accuracy Across All Models: {average_accuracy:.4f}\")\n",
    "\n",
    "ensemble_model = VotingClassifier(estimators=models, voting=\"hard\")\n",
    "scores = cross_val_score(ensemble_model, X_imputed, y, scoring=\"accuracy\")\n",
    "\n",
    "print(\n",
    "    f\"Ensemble Model (Voting): Accuracy: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_encoded = pd.get_dummies(dataframe[categorical_columns], drop_first=True)\n",
    "# X = pd.concat(\n",
    "#     [dataframe[value_columns + ordinal_columns + boolean_columns], X_encoded], axis=1\n",
    "# )\n",
    "# X_clean = X.dropna()\n",
    "\n",
    "# y = dataframe[\"Treatment\"]\n",
    "# y_clean = y[X.index.isin(X_clean.index)]\n",
    "\n",
    "# minority_data = dataframe[(dataframe[\"Treatment\"] != 1) & (dataframe[\"Treatment\"] != 5)]\n",
    "# minority_data = pd.concat([minority_data] * 3)\n",
    "# minority_X_encoded = pd.get_dummies(minority_data[categorical_columns], drop_first=True)\n",
    "# minority_X = pd.concat(\n",
    "#     [\n",
    "#         minority_data[value_columns + ordinal_columns + boolean_columns],\n",
    "#         minority_X_encoded,\n",
    "#     ],\n",
    "#     axis=1,\n",
    "# )\n",
    "# minority_X_clean = minority_X.dropna()\n",
    "\n",
    "# minority_y = minority_data[\"Treatment\"]\n",
    "# minority_y_clean = minority_y[minority_X.index.isin(minority_X_clean.index)]\n",
    "\n",
    "\n",
    "# X_Train = pd.concat([X_clean, minority_X_clean], axis=0)\n",
    "# y_Train = pd.concat([y_clean, minority_y_clean], axis=0)\n",
    "\n",
    "# # print(y_clean.info())\n",
    "# # print(y_Train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Decision Tree Model\n",
    "# param_grid = {\n",
    "#     \"max_depth\": [1, 2, 3, 4, 5, 10],\n",
    "#     \"min_samples_split\": [2, 5, 10],\n",
    "#     \"min_samples_leaf\": [1, 2, 4],\n",
    "# }\n",
    "# grid_search = GridSearchCV(\n",
    "#     DecisionTreeClassifier(), param_grid, cv=cv, scoring=\"accuracy\"\n",
    "# )\n",
    "# grid_search.fit(X_Train, y_Train)\n",
    "# best_params = grid_search.best_params_\n",
    "# # print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# best_max_depth = grid_search.best_params_[\"max_depth\"]\n",
    "# best_min_samples_split = grid_search.best_params_[\"min_samples_split\"]\n",
    "# best_min_samples_leaf = grid_search.best_params_[\"min_samples_leaf\"]\n",
    "\n",
    "# tree_model = DecisionTreeClassifier(\n",
    "#     max_depth=best_max_depth,\n",
    "#     min_samples_split=best_min_samples_split,\n",
    "#     min_samples_leaf=best_min_samples_leaf,\n",
    "# )\n",
    "\n",
    "# tree_model.fit(X_Train, y_Train)\n",
    "# tree_predicted = cross_val_predict(tree_model, X, y, cv=cv)\n",
    "\n",
    "# # Evaluation\n",
    "# print(\"Simple k=\" + str(cv) + \" K fold CV\")\n",
    "# print(\"Decision Tree Model:\")\n",
    "# print(classification_report(y, tree_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_labels = [str(label) for label in tree_model.classes_]\n",
    "# plt.figure(figsize=(135, 90))\n",
    "# plot_tree(tree_model,max_depth=5, feature_names=X.columns, class_names=class_labels, filled=True, rounded=True)\n",
    "# plt.show()\n",
    "\n",
    "# Note that colors are based on the majority class in a leaf (with intensity being an indicator for how large this majority is over the others)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Histogram-based Gradient Boosting Classification Tree Model\n",
    "# boosting_model = HistGradientBoostingClassifier(max_depth=5)\n",
    "# boosting_model.fit(X_Train, y_Train)\n",
    "# boosting_predicted = cross_val_predict(boosting_model, X, y, cv=cv)\n",
    "\n",
    "# # Evaluation\n",
    "# print(\"Simple k=\" + str(cv) + \" K fold CV\")\n",
    "# print(\"Histogram-based Gradient Boosting Classification Tree Model:\")\n",
    "# print(classification_report(y, boosting_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Random forest model\n",
    "# rf_model = RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "# rf_model.fit(X_Train, y_Train)\n",
    "# rf_predicted = cross_val_predict(rf_model, X_clean, y_clean, cv=cv)\n",
    "\n",
    "# # Evaluation\n",
    "# print(\"Simple k=\" + str(cv) + \" K fold CV\")\n",
    "# print(\"Random forest model:\")\n",
    "# print(classification_report(y_clean, rf_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # KNN model\n",
    "# knn_model = KNeighborsClassifier(n_neighbors=7)  \n",
    "# knn_model.fit(X_Train, y_Train)\n",
    "# knn_predicted = cross_val_predict(knn_model, X_clean, y_clean, cv=cv)\n",
    "\n",
    "# # Evaluation\n",
    "# print(\"Simple k=\" + str(cv) + \" K fold CV\")\n",
    "# print(\"K-Nearest Neighbors model:\")\n",
    "# print(classification_report(y_clean, knn_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ensemble model - Duplication\n",
    "# ensemble_model = VotingClassifier(estimators=[\n",
    "#     ('decision_tree', tree_model),\n",
    "#     ('gradient_boosting', boosting_model),\n",
    "#     ('random_forest', rf_model)\n",
    "#     ,('knn',knn_model)\n",
    "# ], voting='hard')\n",
    "\n",
    "# ensemble_predicted = cross_val_predict(ensemble_model, X_clean, y_clean, cv=cv)\n",
    "\n",
    "# print(\"Simple k=\" + str(cv) + \" K fold CV\")\n",
    "# print(\"Ensemble Model:\")\n",
    "# print(classification_report(y_clean, ensemble_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ensemble model - No duplication\n",
    "# # ------------------------------------------------------------------------\n",
    "# # Decision Tree Model\n",
    "# param_grid = {\n",
    "#     \"max_depth\": [1, 2, 3, 4, 5, 10],\n",
    "#     \"min_samples_split\": [2, 5, 10],\n",
    "#     \"min_samples_leaf\": [1, 2, 4],\n",
    "# }\n",
    "# grid_search = GridSearchCV(\n",
    "#     DecisionTreeClassifier(), param_grid, cv=cv, scoring=\"accuracy\"\n",
    "# )\n",
    "# grid_search.fit(X, y)\n",
    "# best_params = grid_search.best_params_\n",
    "# # print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# best_max_depth = grid_search.best_params_[\"max_depth\"]\n",
    "# best_min_samples_split = grid_search.best_params_[\"min_samples_split\"]\n",
    "# best_min_samples_leaf = grid_search.best_params_[\"min_samples_leaf\"]\n",
    "\n",
    "# tree_model = DecisionTreeClassifier(\n",
    "#     max_depth=best_max_depth,\n",
    "#     min_samples_split=best_min_samples_split,\n",
    "#     min_samples_leaf=best_min_samples_leaf,\n",
    "# )\n",
    "\n",
    "# tree_model.fit(X_clean, y_clean)\n",
    "# # ------------------------------------------------------------------------\n",
    "# # Random forest model\n",
    "# rf_model = RandomForestClassifier(max_depth=5)\n",
    "# rf_model.fit(X_clean, y_clean)\n",
    "# # ------------------------------------------------------------------------\n",
    "# # Histogram-based Gradient Boosting Classification Tree Model\n",
    "# boosting_model = HistGradientBoostingClassifier(max_depth=5)\n",
    "# boosting_model.fit(X_clean, y_clean)\n",
    "# # ------------------------------------------------------------------------\n",
    "\n",
    "# ensemble_model = VotingClassifier(estimators=[\n",
    "#     ('decision_tree', tree_model),\n",
    "#     ('gradient_boosting', boosting_model),\n",
    "#     ('random_forest', rf_model)\n",
    "# ], voting='hard')\n",
    "\n",
    "# ensemble_predicted = cross_val_predict(ensemble_model, X_clean, y_clean, cv=cv)\n",
    "\n",
    "# print(\"Simple k=\" + str(cv) + \" K fold CV\")\n",
    "# print(\"Ensemble Model:\")\n",
    "# print(classification_report(y_clean, ensemble_predicted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
